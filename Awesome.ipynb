{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['주야', '요일', '발생지시도', '발생지시군구', '사고유형_대분류', '사고유형_중분류', '법규위반', \n",
    "            '도로형태_대분류', '도로형태', '당사자종별_1당_대분류', '당사자종별_2당_대분류']\n",
    "numerical = ['사상자수', '사망자수', '중상자수', '경상자수','부상신고자수']\n",
    "\n",
    "x_train_num = pd.read_csv('./교통사망사고정보/Kor_Train_교통사망사고정보(12.1~17.6).csv',encoding='cp949', \n",
    "                              usecols=numerical)\n",
    "\n",
    "x_train_cat = pd.read_csv('./교통사망사고정보/Kor_Train_교통사망사고정보(12.1~17.6).csv',encoding='cp949',\n",
    "                               usecols=categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_num = pd.read_csv('./test_kor.csv',encoding='cp949', \n",
    "                              usecols=numerical)\n",
    "\n",
    "x_test_cat = pd.read_csv('./test_kor.csv',encoding='cp949',\n",
    "                               usecols=categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encdoing을 나열해서 만드는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((x_test_cat.dropna(),x_train_cat))\n",
    "# for col in all_data.select_dtypes(include=[np.object]).columns:\n",
    "#     print(col, all_data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iron/.local/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  \n",
      "/home/iron/.local/lib/python3.5/site-packages/ipykernel_launcher.py:3: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for column in all_data.select_dtypes(include=[np.object]).columns:\n",
    "    x_train_cat[column] = x_train_cat[column].astype('category', categories = all_data[column].unique())\n",
    "    x_test_cat[column] = x_test_cat[column].astype('category', categories = all_data[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat = pd.get_dummies(data=x_train_cat)\n",
    "x_test_cat = pd.get_dummies(data=x_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25037, 328)\n",
      "(50, 328)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cat.shape)\n",
    "print(x_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, concatenate, Dropout, Conv2D, MaxPool2D, Embedding, Reshape, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras import metrics\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25037, 328)\n",
      "(25037, 5)\n",
      "(50, 328)\n",
      "(50, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cat.shape)\n",
    "print(x_train_num.shape)\n",
    "print(x_test_cat.shape)\n",
    "print(x_test_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric cases\n",
    "Case1 : ['사망자수','사상자수','경상자수']\n",
    "Case2 = ['사상자수', '중상자수', '부상신고자수']\n",
    "Case3 = ['사상자수', '중상자수', '경상자수' ]\n",
    "Case4 = ['사망자수', '사상자수', '중상자수' ]\n",
    "\n",
    "# Categorical cases\n",
    "Case5 = ['사고유형_대분류', '사고유형_중분류', '법규위반']\n",
    "Case6 = ['도로형태_대분류', '도로형태', '당사자종별_1당_대분류']\n",
    "Case7 = ['도로형태_대분류', '도로형태', '당사자종별_2당_대분류']\n",
    "Case8 = ['도로형태_대분류', '도로형태', '당사자종별_1당_대분류', '당사자종별_2당_대분류']\n",
    "Case11 = ['발생지시도', '발생지시군구']\n",
    "Case12 = ['요일', '사고유형_대분류', '사고유형_중분류']\n",
    "Case13 = ['요일', '사고유형_중분류', '법규위반', '도로형태_대분류']\n",
    "\n",
    "# Mixed cases\n",
    "Case9 = ['사망자수', '사상자수', '발생지시군구']\n",
    "Case10 = ['중상자수', '경상자수', '발생지시군구']\n",
    "Case14 = ['사망자수', '사상자수', '주야', '당사자종별_1당_대분류']\n",
    "Case15 = ['사상자수', '중상자수', '주야', '도로형태']\n",
    "\n",
    "Cases = [Case1, Case2, Case3, Case4, Case5, Case6, Case7, Case8, Case11, Case12, Case13,\n",
    "        Case9, Case10, Case14, Case15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 데이터 Case 함수\n",
    "\n",
    "def numeric_case(case, start, end):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    print(\"Case:\", case)\n",
    "    X = x_train_num.drop(columns=case)\n",
    "    X_test = x_test_num.drop(columns=case)\n",
    "    \n",
    "    case_copy=case.copy()\n",
    "    if '사상자수' in case:       \n",
    "        case_copy.remove('사상자수')\n",
    "    print('사상자제거:', case_copy)\n",
    "    Y = x_train_num[case_copy].values\n",
    "    \n",
    "    # 수치형 데이터와 범주형 데이터 합치기\n",
    "    X = pd.concat([X, x_train_cat], axis=1).values\n",
    "    X_test = pd.concat([X_test, x_test_cat],axis=1).values\n",
    "    \n",
    "    print(X)\n",
    "    print(Y)\n",
    "    \n",
    "    # 모델 정의\n",
    "    num_input = Input(shape=(len(X[0]),), name='num_input')\n",
    "    x = Dense(512, activation='relu')(num_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    num_output = Dense(len(Y[0]), name='num_output')(x)\n",
    "\n",
    "    model = Model(inputs=num_input, outputs=num_output)\n",
    "\n",
    "    model.compile(optimizer='sgd',\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                        patience=25, \n",
    "                                        verbose=1, \n",
    "                                        factor=0.5, \n",
    "                                        min_lr=0.00000001)\n",
    "\n",
    "    callbacks = [\n",
    "    #         learning_rate_reduction, # learning_rate를 점차 감소시킴으로서 최적값에 접근하는 방식\n",
    "        EarlyStopping('val_loss', patience=5)# val_loss이 최적값에서 멀어지는 순간 epoch가 남아도 학습 중지\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X, Y, epochs=50, batch_size=128, callbacks=callbacks,validation_split=0.2 )\n",
    "    \n",
    "    \n",
    "    # make a prediction\n",
    "    Y_test = model.predict(X_test[start:end+1])\n",
    "    \n",
    "    # show the inputs and predicted outputs\n",
    "    print(\"X=%s, Predicted=%s\" % (X_test[range(start, end+1)],  Y_test ))\n",
    "    del model\n",
    "    \n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터 Case 함수\n",
    "\n",
    "def categorical_case(case, start, end):\n",
    "    col_name = [] # ex. '사고유형_대분류_차대차', '사고유형_대분류_차대사람', '사고유형_대분류_차량단독'\n",
    "    label_name = [] #  ex. '차대차', '차대사람', '차량단독\n",
    "    \n",
    "    for col in case:\n",
    "        label_name.extend(all_data[col].unique()) \n",
    "        for name in all_data[col].unique():\n",
    "            col_name.append(col+'_'+name)\n",
    "    \n",
    "    print('col_name:',col_name)\n",
    "    print('label_name:', label_name)\n",
    "                \n",
    "    Y = x_train_cat[col_name].values\n",
    "    X = x_train_cat.drop(columns=col_name)\n",
    "    X = pd.concat([X, x_train_num], axis=1).values\n",
    "\n",
    "    X_test = x_test_cat.drop(columns=col_name)\n",
    "    X_test = pd.concat([X_test, x_test_num],axis=1).values\n",
    "\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    cat_input = Input(shape=(len(X[0]),), name='cat_input')\n",
    "    x = Dense(512, activation='relu')(cat_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    cat_output = Dense(len(Y[0]), activation='sigmoid', name='cat_output')(x)\n",
    "\n",
    "    model = Model(inputs=cat_input, outputs=cat_output)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                                patience=25, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00000001)\n",
    "\n",
    "    callbacks = [\n",
    "        learning_rate_reduction, # learning_rate를 점차 감소시킴으로서 최적값에 접근하는 방식\n",
    "        EarlyStopping('val_loss', patience=10), # val_loss이 최적값에서 멀어지는 순간 epoch가 남아도 학습 중지\n",
    "        ]\n",
    "\n",
    "    history = model.fit(X, Y, epochs=50, batch_size=128, callbacks=callbacks,validation_split=0.2 )\n",
    "    \n",
    "    \n",
    "    # make a prediction\n",
    "    Y_test = model.predict(X_test[range(start, end+1)])\n",
    "    \n",
    "    # show the inputs and predicted outputs\n",
    "#     print(\"X=%s, Predicted=%s\" % (X_test[range(start, end+1)],  Y_test ))\n",
    "    \n",
    "            \n",
    "    result = []\n",
    "    '''\n",
    "    예시 출력:\n",
    "        사고유형_대분류 : 차량단독\n",
    "        사고유형_중분류 : 공작물충돌\n",
    "        법규위반 : 안전운전 의무 불이행\n",
    "    '''\n",
    "    for val in Y_test: \n",
    "        x_list = list(val)\n",
    "        label_name_x = label_name.copy()\n",
    "        temp = []\n",
    "        for col in case:\n",
    "            print(col, ':', label_name_x[x_list.index(max(x_list[0:len(all_data[col].unique())]))] )\n",
    "            temp.append(label_name_x[x_list.index(max(x_list[0:len(all_data[col].unique())]))])\n",
    "            del x_list[:len(all_data[col].unique())]\n",
    "            del label_name_x[:len(all_data[col].unique())]\n",
    "        result.append(temp)\n",
    "        print()\n",
    "    \n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 믹스형 데이터 Case 함수\n",
    "\n",
    "def mix_case(case, n, start, end):\n",
    "    '''\n",
    "    case: Case에 해당되는 컬럼이 담긴 배열\n",
    "    n: 범주형 데이터 수\n",
    "    start: 해당 Case 테스트의 시작 인덱스\n",
    "    end: 해당 Case 테스트의 마지막 인덱스\n",
    "    '''\n",
    "    \n",
    "    case_copy=case.copy()\n",
    "    \n",
    "    # categorical\n",
    "    col_name = []\n",
    "    label_name = []\n",
    "    cat_name = case_copy[-n:]\n",
    "    \n",
    "    for col in case_copy[-n:]:\n",
    "        label_name.extend(all_data[col].unique()) \n",
    "        for name in all_data[col].unique():\n",
    "            col_name.append(col+'_'+name)\n",
    "    \n",
    "    Y_cat = x_train_cat[col_name].values\n",
    "    X1 = x_train_cat.drop(columns=col_name)\n",
    "    X_test1 = x_test_cat.drop(columns=col_name)\n",
    "    \n",
    "    # categorical columns 삭제\n",
    "    del case_copy[-n:]\n",
    "    \n",
    "    # numerical\n",
    "    X2 = x_train_num.drop(columns=case_copy)\n",
    "    X_test2 = x_test_num.drop(columns=case_copy)\n",
    "    if '사상자수' in case: \n",
    "        case_copy.remove('사상자수')\n",
    "    Y_num = x_train_num[case_copy].values\n",
    "    \n",
    "    X = pd.concat([X1, X2], axis=1).values\n",
    "    X_test = pd.concat([X_test1, X_test2],axis=1).values\n",
    "    print(Y_num.shape)\n",
    "    print(len(Y_num[0]))\n",
    "    \n",
    "    cat_input = Input(shape=(len(X[0]),), name='cat_input')\n",
    "    x = Dense(512, activation='relu')(cat_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    cat_output = Dense(len(Y_cat[0]), activation='softmax', name='cat_output')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    num_output = Dense(len(Y_num[0]), name='num_output')(x)\n",
    "\n",
    "    model = Model(inputs=cat_input, outputs=[cat_output, num_output])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'cat_output': 'categorical_crossentropy', 'num_output': 'mse'},\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=25, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00000001)\n",
    "\n",
    "    callbacks = [\n",
    "        learning_rate_reduction, # learning_rate를 점차 감소시킴으로서 최적값에 접근하는 방식\n",
    "        EarlyStopping('val_loss', patience=20), # val_loss이 최적값에서 멀어지는 순간 epoch가 남아도 학습 중지\n",
    "    ]\n",
    "\n",
    "    history = model.fit(X, {'cat_output':Y_cat, 'num_output':Y_num}, epochs=50, batch_size=128, callbacks=callbacks,validation_split=0.2 )\n",
    "    \n",
    "    \n",
    "    # make a prediction\n",
    "    Y_test = model.predict(X_test[start:end+1])\n",
    "    \n",
    "    # show the inputs and predicted outputs\n",
    "    print(\"X=%s, Predicted=%s\" % (X_test[start:end+1],  Y_test ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for val in Y_test[0]:\n",
    "        x_list = list(val)\n",
    "        label_name_x = label_name.copy()\n",
    "        for col in cat_name:\n",
    "            print(col, ':', label_name_x[x_list.index(max(x_list[0:len(all_data[col].unique())]))] )\n",
    "            # 출력한 Column과 데이터 삭제\n",
    "            del x_list[:len(all_data[col].unique())]\n",
    "            del label_name_x[:len(all_data[col].unique())]\n",
    "        print()\n",
    "        \n",
    "    for num in Y_test[1]:\n",
    "        print(case_copy, ':', num)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: ['사망자수', '사상자수', '경상자수']\n",
      "사상자제거: ['사망자수', '경상자수']\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [2 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [5 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n",
      "[[1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Train on 20029 samples, validate on 5008 samples\n",
      "Epoch 1/50\n",
      "20029/20029 [==============================] - 1s 52us/step - loss: 0.6932 - acc: 0.9441 - val_loss: 0.6104 - val_acc: 0.9507\n",
      "Epoch 2/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.6302 - acc: 0.9528 - val_loss: 0.5900 - val_acc: 0.9373\n",
      "Epoch 3/50\n",
      "20029/20029 [==============================] - 1s 27us/step - loss: 0.6145 - acc: 0.9456 - val_loss: 0.5859 - val_acc: 0.9355\n",
      "Epoch 4/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.6092 - acc: 0.9474 - val_loss: 0.5839 - val_acc: 0.9377\n",
      "Epoch 5/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5992 - acc: 0.9459 - val_loss: 0.5648 - val_acc: 0.9309\n",
      "Epoch 6/50\n",
      "20029/20029 [==============================] - 1s 27us/step - loss: 0.5896 - acc: 0.9445 - val_loss: 0.5653 - val_acc: 0.9369\n",
      "Epoch 7/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5962 - acc: 0.9459 - val_loss: 0.5761 - val_acc: 0.9415\n",
      "Epoch 8/50\n",
      "20029/20029 [==============================] - 1s 25us/step - loss: 0.5863 - acc: 0.9469 - val_loss: 0.5635 - val_acc: 0.9409\n",
      "Epoch 9/50\n",
      "20029/20029 [==============================] - 1s 25us/step - loss: 0.5826 - acc: 0.9472 - val_loss: 0.5483 - val_acc: 0.9299\n",
      "Epoch 10/50\n",
      "20029/20029 [==============================] - 1s 25us/step - loss: 0.5839 - acc: 0.9459 - val_loss: 0.5736 - val_acc: 0.9447\n",
      "Epoch 11/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5809 - acc: 0.9456 - val_loss: 0.5439 - val_acc: 0.9345\n",
      "Epoch 12/50\n",
      "20029/20029 [==============================] - 0s 25us/step - loss: 0.5863 - acc: 0.9441 - val_loss: 0.5647 - val_acc: 0.9467\n",
      "Epoch 13/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5766 - acc: 0.9460 - val_loss: 0.5356 - val_acc: 0.9311\n",
      "Epoch 14/50\n",
      "20029/20029 [==============================] - 1s 28us/step - loss: 0.5774 - acc: 0.9434 - val_loss: 0.5459 - val_acc: 0.9431\n",
      "Epoch 15/50\n",
      "20029/20029 [==============================] - 0s 25us/step - loss: 0.5667 - acc: 0.9437 - val_loss: 0.5610 - val_acc: 0.9493\n",
      "Epoch 16/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5570 - acc: 0.9446 - val_loss: 0.5327 - val_acc: 0.9337\n",
      "Epoch 17/50\n",
      "20029/20029 [==============================] - 1s 25us/step - loss: 0.5579 - acc: 0.9442 - val_loss: 0.5249 - val_acc: 0.9337\n",
      "Epoch 18/50\n",
      "20029/20029 [==============================] - 1s 27us/step - loss: 0.5681 - acc: 0.9447 - val_loss: 0.5292 - val_acc: 0.9405\n",
      "Epoch 19/50\n",
      "20029/20029 [==============================] - 0s 25us/step - loss: 0.5526 - acc: 0.9438 - val_loss: 0.5593 - val_acc: 0.9481\n",
      "Epoch 20/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5585 - acc: 0.9458 - val_loss: 0.5299 - val_acc: 0.9423\n",
      "Epoch 21/50\n",
      "20029/20029 [==============================] - 1s 28us/step - loss: 0.5812 - acc: 0.9453 - val_loss: 0.5460 - val_acc: 0.9477\n",
      "Epoch 22/50\n",
      "20029/20029 [==============================] - 1s 26us/step - loss: 0.5420 - acc: 0.9425 - val_loss: 0.5371 - val_acc: 0.9469\n",
      "X=[[0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], Predicted=[[ 0.99262625  0.35796607]\n",
      " [ 0.9594538  -0.0098708 ]]\n"
     ]
    }
   ],
   "source": [
    "Case1_pre = numeric_case(Case1, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99262625,  0.35796607],\n",
       "       [ 0.9594538 , -0.0098708 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Case1_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set result array to each cases\n",
    "def setResult(arr, predict, case):\n",
    "    result_arr = arr\n",
    "    \n",
    "    \n",
    "    if case == [0,1]:\n",
    "        for row in case:\n",
    "            result_arr[row, 2] = predict[row, 0] \n",
    "            result_arr[row, 5] = predict[row, 1] \n",
    "            result_arr[:, 3] = result_arr[row, 2].sum() + result_arr[row, 4:7].sum()\n",
    "            print(result_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['야간' '금' 0.99262625 1.350592315196991 0.0 0.35796607 0.0 '경기' '화성시'\n",
      "  '차대차' '측면충돌' '중앙선 침범' '단일로' '기타단일로' '승용차' '승합차']\n",
      " ['야간' '금' nan 1.350592315196991 0.0 nan 0.0 '전남' '영암군' '차대사람' '차도통행중'\n",
      "  '과속' '단일로' '기타단일로' '승용차' '보행자']]\n",
      "[['야간' '금' 0.99262625 0.9495830247178674 0.0 0.35796607 0.0 '경기' '화성시'\n",
      "  '차대차' '측면충돌' '중앙선 침범' '단일로' '기타단일로' '승용차' '승합차']\n",
      " ['야간' '금' 0.9594538 0.9495830247178674 0.0 -0.009870796 0.0 '전남' '영암군'\n",
      "  '차대사람' '차도통행중' '과속' '단일로' '기타단일로' '승용차' '보행자']]\n"
     ]
    }
   ],
   "source": [
    "x_test = pd.read_csv('./test_kor.csv',encoding='cp949')\n",
    "\n",
    "# setResult(each case array, predict array, start to end in each case):\n",
    "setResult(x_test.loc[:1].values, Case1_pre, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set result array to each cases\n",
    "def setResult(arr, predict, case):\n",
    "    result_arr = arr\n",
    "    \n",
    "    if case == [int(x) for x in range(10,20)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 9] = predict[row, 0] \n",
    "            result_arr[row, 10] = predict[row, 1] \n",
    "            result_arr[row, 11] = predict[row, 2] \n",
    "            \n",
    "    elif case == [int(x) for x in range(20,23)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 12] = predict[row, 0] \n",
    "            result_arr[row, 13] = predict[row, 1] \n",
    "            result_arr[row, 14] = predict[row, 2] \n",
    "            \n",
    "    elif case == [int(x) for x in range(23,26)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 12] = predict[row, 0] \n",
    "            result_arr[row, 13] = predict[row, 1] \n",
    "            result_arr[row, 15] = predict[row, 2] \n",
    "    \n",
    "    elif case == [int(x) for x in range(26,30)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 12] = predict[row, 0] \n",
    "            result_arr[row, 13] = predict[row, 1] \n",
    "            result_arr[row, 14] = predict[row, 2] \n",
    "            result_arr[row, 15] = predict[row, 3] \n",
    "    \n",
    "    elif case == [int(x) for x in range(35,40)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 7] = predict[row, 0] \n",
    "            result_arr[row, 8] = predict[row, 1] \n",
    "        \n",
    "        \n",
    "    elif case == [int(x) for x in range(40,42)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 1] = predict[row, 0] \n",
    "            result_arr[row, 9] = predict[row, 1] \n",
    "            result_arr[row, 10] = predict[row, 2] \n",
    "        \n",
    "    elif case == [int(x) for x in range(42,45)]:\n",
    "        for row in range(len(predict)):\n",
    "            result_arr[row, 1] = predict[row, 0] \n",
    "            result_arr[row, 10] = predict[row, 1] \n",
    "            result_arr[row, 11] = predict[row, 2] \n",
    "            result_arr[row, 12] = predict[row, 3] \n",
    "            \n",
    "            \n",
    "    print(result_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['주간' '수' 1.0 1.0 0.0 0.0 0.0 '경기' '부천시' '차대차' '추돌' '안전운전 의무 불이행' '교차로'\n",
      "  '교차로내' '승합차' '이륜차']\n",
      " ['주간' '월' 1.0 1.0 0.0 0.0 0.0 '대전' '동구' '차량단독' '공작물충돌' '안전운전 의무 불이행'\n",
      "  '단일로' '기타단일로' '특수차' '없음']\n",
      " ['야간' '토' 2.0 2.0 0.0 0.0 0.0 '충남' '보령시' '차대사람' '횡단중' '안전운전 의무 불이행'\n",
      "  '단일로' '기타단일로' '승용차' '보행자']]\n"
     ]
    }
   ],
   "source": [
    "setResult(x_test.loc[42:44].values, Case13_pre, [int(x) for x in range(42,45)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
